{"chunk_id": "DOC0038__00000", "doc_id": "DOC0038", "chunk_index": 0, "text": "Topic: Ensemble Learning Techniques\nDr. Daphne Nyachaki Bitalo\nDepartment of Computing & Technology\nFaculty of Engineering, Design & Technology\nDSC3108: Big Data Mining and Analytics\nLecture 09 (BSCS_3:1)\n\nCOURSE OVERVIEW\nBig Data\nFundamentals and Mining\nData Analytics\nPredictive analytics\nDescriptive analytics\nData Mining Techniques\nArchitectures, storage\nof data\n\nLecture Objectives and Learning outcomes\nThe Objectives of this lecture are to:\n❑Learn how to improve the predictive performance of an ML\nmodel\n❑Learn about the different ensemble methods and how they are\ntrained and combined\nBy the end of this lecture, students should be able to:\n❑Get practical experience working with ensemble techniques to\nimprove classification and regression predictive models\n\nLecture Overview\nIntroduction to ensembles\nEnsemble methods\nCombining ensembles\nHands-on practical\nPros, cons of ensembles\n\nWhy use ensembles?\n●ML/ predictive models need constant updates as new data becomes\navailable for accurate and reliable predictions. Therefore, ensemble\ntechniques aid in boosting this accuracy.\nConcept\nExplanation\nAnalogy\nDefinition\nEnsemble learning is a meta-algorithm that combines the\npredictions from multiple base models (called weak learners or\nbase estimators) to produce a single, superior prediction.\nAsking a panel of experts (diverse\nperspectives) for an opinion rather than\nrelying on a single individual. The panel's\naverage/majority decision is often better.\nGoal\nTo improve predictive performance (accuracy, robustness) and\nreduce common issues like overfitting and high variance or high\nbias that a single model might exhibit.\nWeak Learner\nA model (e.g., a simple Decision Tree) that performs slightly\nbetter than random chance. Ensembles make a \"forest\" from\nthese \"trees.\"\n\nModel errors and ensemble correction\n❑Errors emerging from ML models can be broken down into three\ncomponents mathematically: Bias + Variance + Irreducible error\n❑Bias: quantifies how much, on an average, the predicted values are\ndifferent from the actual value. A high bias error means we have an\nunderperforming model that keeps missing essential trends.\n❑Variance: quantifies how the predictions made on the same observation\ndiffer. A high variance model will over-fit on your training population\nand perform poorly on any observation beyond training.\n\nModel errors and ensemble correction\n❑As model complexity increases, you\nwill see a reduction in error due to\nlower bias in the model. However,\nthis only happens until a particular\npoint. As you continue to make your\nmodel more complex, you end up\nover-fitting your model, and hence\nyour model will start suffering from\nthe high variance.\n\nEnsemble methods\n❑Ensemble methods are categorised based on how the base\nmodels are trained and combined.\n❑The methods are also categorised for their purpose (either\nto reduce variance or to reduce bias)\n❑Averaging/Parallel ensemble methods reduce variance\n❑Boosting/Sequential ensemble methods reduce bias", "word_count": 435, "start_char": 0, "end_char": 2989}
{"chunk_id": "DOC0038__00001", "doc_id": "DOC0038", "chunk_index": 1, "text": "nsemble methods\n❑Ensemble methods are categorised based on how the base\nmodels are trained and combined.\n❑The methods are also categorised for their purpose (either\nto reduce variance or to reduce bias)\n❑Averaging/Parallel ensemble methods reduce variance\n❑Boosting/Sequential ensemble methods reduce bias\n\nEnsemble methods: Averaging techniques\n❑These methods train the base models/learners independently and in\nparallel. They reduce variance and overfitting in the final model.\nMethod\nTraining Strategy\nCombination\nKey Benefit\n1. Bagging (Bootstrap\nAggregating)\nEach weak learner is trained on a\ndifferent, random bootstrap sample\n(sampling with replacement) of the\noriginal training data.\nAveraging (for regression) or\nMajority Voting (for\nclassification).\nReduces variance (overfitting)\nby training models on slightly\ndifferent datasets, thereby\ndecorrelating their errors.\n2. Random Forest (RF)\nAn extension of Bagging. Each tree is\nbuilt on a bootstrap sample, AND at\neach node split, only a random\nsubset of features is considered.\nMajority Voting/Averaging.\nFurther reduces correlation\nbetween trees, making the\nensemble much more robust\nthan standard Bagging. High\nperformance and feature\nimportance readily available.\n\nExample bootstrapping/Bagging\n❑In bagging, a random sample of\ndata from the training set is\nselected with replacement, which\nenables the duplication of sample\ninstances in a set. Below are the\nmain steps involved in bagging:\n•Generation of multiple bootstrap\nresamples.\n•Running an algorithm on each resample to\nmake predictions.\n•Combining the predictions by taking the\naverage of the predictions or taking the\nmajority vote (for classification).\n\nEnsemble methods: Boosting techniques\n❑These methods train the base models/learners sequentially (in series).\nEach new learner attempts to correct the errors of the previous learners.\nThey reduce bias and model predictive accuracy.\nMethod\nTraining Strategy\nCombination\nKey Benefit\n1. AdaBoost (Adaptive Boosting)\nEach successive learner is trained\non the same data, but the data\npoints that were misclassified by\nthe previous model are given\nhigher weights.\nWeighted Majority Voting, where\nbetter-performing models (lower\nerror) are given higher weight in\nthe final vote.\nFocuses training iteratively on\nhard examples (reducing bias).\n2. Gradient Boosting Machines\n(GBM)\nEach successive learner is trained\nto predict the residual error (or\ngradient) of the previous\nensemble of trees.\nPredictions are summed up:\nY^=∑k=1Kfk(X).\nHighly effective for complex\nproblems. State-of-the-art\nperformance in many tabular data\ntasks.\n3. XGBoost, LightGBM, CatBoost\nHighly optimized, scalable\nversions of GBM (e.g., parallel\nprocessing for tree construction,\nhandling missing values).\nSummation of weighted tree\npredictions.\nComputational efficiency and\nsuperior predictive power on\nmassive datasets.\n\nExample Boosting techniques\n❑The prediction of the current\nmodel is transferred to the next\none. Each model iteratively\nfocuses attention on the\nobservations that are\nmisclassified by its predecessors.", "word_count": 426, "start_char": 2684, "end_char": 5753}
{"chunk_id": "DOC0038__00002", "doc_id": "DOC0038", "chunk_index": 2, "text": "ummation of weighted tree\npredictions.C\n\nomputational efficiency and\nsuperior predictive power on\nmassive datasets.E\n\nxample Boosting techniques\n❑The prediction of the current\nmodel is transferred to the next\none.E\n\nach model iteratively\nfocuses attention on the\nobservations that are\nmisclassified by its predecessors.\n\nEnsemble combination approaches\nCombination:\n●This is the approach in which the predictions made by base models are aggregated\nby the ensemble technique:\n❑Majority Voting (Classification): The final class is the one predicted by the majority\nof the individual models.\n❑Averaging (Regression): The final prediction is the arithmetic mean of the individual\nmodels' predictions.\n❑Stacking (Stacked Generalization): Stacking (Stacked Generalization): Uses\nheterogenous base learners. The predictions from the base learners are stacked\ntogether and are used as the input to train the meta learner to produce more robust\npredictions. The meta learner is then used to make final predictions\n\nPros and Cons of employing ensembles\nAspect\nPros\nCons\nPerformance\nTypically achieves higher accuracy\nthan any single base model, especially\nBoosting methods (XGBoost).\nComputationally expensive and\ntime-consuming to train,\nespecially for large datasets.\nRobustness\nReduces overfitting (Bagging/RF) and\nhandles noisy data better due to the\ncollective decision-making.\nLoss of interpretability. A single\nDecision Tree is easy to explain;\na Random Forest of 500 trees is a\nblack box (confusing).\nEase of Use\nModern implementations (e.g., in\nscikit-learn or R's caret) are easy to\nimplement with few lines of code.\nHyperparameter complexity:\nMore parameters to tune (e.g.,\nnumber of estimators, learning\nrate, tree depth).\n\nReading Assignment\n❖Sources of errors in predictive/ML models\n❖Advantages and Disadvantages of different ensemble\ntechniques\n❖Advantages and Disadvantages of ensemble combination\napproaches\n❖Ensemble techniques for clustering mining/unsupervised\nMachine Learning\n❖Applications of various ensemble techniques\n\n@ugandachristianuniversity\n@UCUniversity\n@UgandaChristianUniversity\nP.O. Box 4 Mukono, Uganda\nTel: 256-312-350800\nEmail: info@ucu.ac.ug.\nUganda Christian University\nhttps://cse.ucu.ac.ug/\n@ucu_ComputEng\n@ucucomputeng\nTel: +256 (0) 312 350 863 | WhatsApp: +256 (0) 708 114 300\nDepartment of Computing & Technology\nFACULTY OF ENGINEERING, DESIGN AND TECHNOLOGY\nEmail: dct-info@ucu.ac.ug", "word_count": 322, "start_char": 5434, "end_char": 7853}
