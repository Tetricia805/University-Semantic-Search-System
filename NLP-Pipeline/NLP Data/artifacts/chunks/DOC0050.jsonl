{"chunk_id": "DOC0050__00000", "doc_id": "DOC0050", "chunk_index": 0, "text": "FACULTY OF ENGINEERING, DESIGN AND TECHNOLOGY\nDEPARTMENT OF COMPUTING AND TECHNOLOGY\n\nADVENT 2025 SEMESTER EXAM\nPROGRAM: BACHELOR OF SCIENCE IN COMPUTER SCIENCE (BSCS)\nYEAR: 3\n\nSEMESTER: 1\nCOURSE CODE: CSC3116 EXAMINATION TYPE: PROJECT-BASED\nCOURSE NAME: MACHINE LEARNING\nEXAM DATE: DECEMBER 2025\nTIME ALLOWED: 2 Weeks\n\nINSTRUCTIONS TO CANDIDATES\n1. This paper consists of four (4) questions. Attempt all questions.\n2. Prepare and submit one consolidated report (PDF or Word) that clearly presents\nyour methodology, results, analysis, and conclusions for each question.\n3. For each question, submit all supporting files (datasets, code, trained models,\nand output files) as specified at the end of each question.\n4. Upload your complete submission (report + code + models + output files) to your\nMoodle account within 2 weeks of receiving this exam. Late submissions will not\nbe accepted.\n5. This is an individual and formal examination. Collaboration, discussion, or\nsharing of code/results with others is strictly prohibited. Any form of plagiarism\nor collusion will result in loss of marks for all parties involved.\n6. Use k-fold cross-validation for model evaluations to ensure robustness.\n7. Visualize your results (for example, confusion matrices, error plots, learning\ncurves, etc.) whenever possible.\n8. Include clear explanations of design choices, hyperparameter tuning,\nencountered challenges, and solutions.\n9. Bonus marks will be awarded for comprehensive and well-structured reports, high\nmodel performance, and clarity and technical coherence in writing.", "word_count": 228, "start_char": 0, "end_char": 1569}
{"chunk_id": "DOC0050__00001", "doc_id": "DOC0050", "chunk_index": 1, "text": "se k-fold cross-validation for model evaluations to ensure robustness.\n7.V\n\nisualize your results (for example, confusion matrices, error plots, learning\ncurves, etc.) whenever possible.\n8.I\n\nnclude clear explanations of design choices, hyperparameter tuning,\nencountered challenges, and solutions.\n9.B\n\nonus marks will be awarded for comprehensive and well-structured reports, high\nmodel performance, and clarity and technical coherence in writing.\n\nQUESTION 1 (25%)\nIn this question, you are required to design and evaluate a random forest classifier\nfor the task of fish disease classification. The dataset contains features extracted\nfrom fish images, including texture features such as entropy, contrast, energy,\nhomogeneity, correlation, and dissimilarity, color features including average RGB\npixel values, as well as statistical features such as mean, standard deviation,\nvariance, kurtosis, and skewness. The target variable represents ten fish disease\nclasses, encoded as integers from 0 to 9. Your task is to train a random forest\nclassifier on this dataset and evaluate its performance using standard classification\nmetrics. Additionally, you are required to compare the performance of the random\nforest model with that of other classifiers. You are also expected to analyze the\neffect of dimensionality reduction or feature selection on classifier performance,\nmodel interpretability, and computational efficiency.\na) Use the training dataset fish_disease_train.csv (last column = class label) to\ntrain a random forest classifier. Save the trained model file as model_1.pkl.\n(5 marks)\nb) Load the trained model and test it on dataset fish_disease_test.csv to obtain\nmodel predictions. (2 marks)\nc) Evaluate the trained model on precision, recall, F1-score, and accuracy. (4\nd) Apply PCA or any filter feature selection method of your choice and compare\nthe model’s performance on test data, before and after dimensionality\nreduction. (4 marks)\ne) Compare the classification results of the random forest on test data with\ndecision tree and KNN models, in terms of accuracy, precision, recall, F1score and receiver operating characteristic - area under the curve (ROC-AUC),\nwith visualization. (5 marks)\nf) Discuss the results in detail including class-wise performance and the\nimplications of the observed performance trend. (3 marks)\ng) Discuss any model hyperparameter settings, observations, and conclusion. (2\nh) Submit the following files:\n1) mode_1.pkl (trained random forest model).\n\n2) code_source1.ipynb or code_source1.py (source code).\n3) Report section for question 1.", "word_count": 371, "start_char": 1120, "end_char": 3712}
{"chunk_id": "DOC0050__00002", "doc_id": "DOC0050", "chunk_index": 2, "text": "QUESTION 2 (25%)\nIn this question, you are required to design and evaluate linear regression models\nfor a multi-output regression task using the energy efficiency dataset. Specifically,\nyou are required to predict the heating load and cooling load of buildings based on\neight building features, including relative compactness, surface area, wall area, roof\narea, overall height, orientation, glazing area, and glazing area distribution. You are\nrequired to evaluate the model performance separately for each target variable,\nusing appropriate regression metrics, and compare the linear regression model with\nseveral other regression techniques.\na) Use the energy efficiency dataset_train.csv (last two columns = target values)\nto train two linear regression models to predict heating load and cooling load.\nSave the trained models as model1_2.pkl and model2_2.pkl. (4 marks)\nb) Load and test the trained models on the test set, the energy efficiency\ndataset_test.csv. (3 marks)\nc) Visualize two plots of the predicted data against one of the input features\nand include the regression line. (3 marks)\nd) Evaluate the two models on test data using MSE, MAE, RMSE, MAPE, and R2\nscore and report the metrics in a results table. (5 marks)\ne) Discuss the results obtained in part (d), highlighting their implications for the\ntarget variables. Identify which features have the strongest impact on each\ntarget and describe any notable patterns or insights you observe. (Hint: you\nmay use the Pearson correlation coefficient to determine which features most\nstrongly influence each target). (4 marks)\nf) Compare the performance of the linear regressors with the following models:\n1) Polynomial regression\n2) Ridge regression\n3) Lasso regression\n4) Support vector regression (SVR) (4 marks)\ng) Discuss the model hyperparameter settings, observations, and conclusion. (2\n\nh) Submit the following files:\n1) Model1_2.pkl and Model2_2.pkl (trained linear regressors).\n2) code_source2.ipynb or code_source2.py (source code).\n3) Report section for question 2.\nQUESTION 3 (25%)\nIn this question, you are required to design and evaluate an XGBoost ensemble\nclassifier for the MNIST digit recognition dataset.\na) Use the training dataset “mnist_train.csv” (first column = class label) to train\nan XGBoost Classifier. Save the model as “model_3.pkl”. (5 marks)\nb) Fine-tune the critical hyperparameters such as learning rate, max_depth,\nn_estimators, and justify your choices. (5 marks)\nc) Test your trained classifier on “mnist_test.csv” and evaluate your model using\nprecision, recall, F1-score, and accuracy. (5 marks)\nd) Compare results with any baseline models such as logistic regression and\ndecision tree, and ensemble models such as Adaptive Boosting (AdaBoost) and\nGradient Boosting Machines (GBM). (6 marks)\ne) Discuss any observations made and conclusion. (4 marks)\nf) Submit the following files:\n1) model_3.pkl (trained XGBoost model).\n2) code_source3.ipynb or code_source3.py (source code).\n3) Report section for question 3.", "word_count": 447, "start_char": 3712, "end_char": 6729}
{"chunk_id": "DOC0050__00003", "doc_id": "DOC0050", "chunk_index": 3, "text": "QUESTION 4 (25%)\nIn this question, you are required to apply feature selection using mutual information\nto identify the most relevant predictors for ovarian cancer classification on the\novarian cancer dataset.csv. You will then use the selected features to train and\nevaluate a decision tree classifier, and analyze how feature selection affects the\nmodel’s performance.\na) Apply mutual information to select the top 44 most relevant features, and\nsave the resulting dataset as selected_subset.csv. (4 marks)\nb) Train a decision tree classifier on selected_subset.csv dataset. Save the\nmodel as model1_4.pkl. (4 marks)\n\nc) Evaluate the model’s performance (precision, recall, F1-score, accuracy) on\na held-out test split. (4 marks)\nd) Train another decision tree classifier on the overall ovarian_cancer_train.csv\ndataset. Save the model as model2_4.pkl. (4 marks)\ne) Test model2_4.pkl on ovarian_cancer_test.csv and compare its performance\nwith that obtained for model1_4.pkl in (c) above. (5 marks)\nf) Discuss your findings and interpret whether feature selection improved model\ngeneralization. (4 marks)\ng) Submit the following files:\n1) selected_subset.csv (file containing selected 42 features).\n2) model1_4.pkl and model2_4.pkl (trained decision tree models).\n3) code_source4.ipynb or code_source4.py – source code.\n4) Report section for question 4.\n\n~End of Examination~", "word_count": 195, "start_char": 6729, "end_char": 8106}
