{"chunk_id": "DOC0112__00000", "doc_id": "DOC0112", "chunk_index": 0, "text": "Techniques for linear systems of equations\nWe consider the following system of n linear equations in n unknowns.\nai1x1 + ai2 + · · · + ainxn = bi, i = 1, 2, . . ., n.\nWhich can be presented in matrix form as\n(81)\nwhere\nA =\n⎛\n⎜\n⎜\n⎜\n⎝\na11\na12\n· · · a1n\na21\na22\n· · · a2n\n...\n...\n...\nan1\nan2\n· · · ann\n⎞\n⎟\n⎟\n⎟\n⎠\nis the coefficient matrix, x = (x1, x2, · · · , xn)T is the vector of unknowns and b =\n(b1, b2, · · · , bn)T is the known right hand side. The augmented matrix for the system\nis\ñA =\n⎛\n⎜\n⎜\n⎜\n⎝\na11\na12\n· · · a1n\nb1\na21\na22\n· · · a2n\nb2\n...\n...\n...\n...\nan1\nan2\n· · · ann\nbn\n⎞\n⎟\n⎟\n⎟\n⎠\n(82)\nIn this section we discuss methods of solving such systems other than using the\ninverse of A which is an expensive process. The methods are grouped into two\ncategories: direct methods and indirect methods.\n7.1\nDirect methods obtain the solution in a finite number of steps and return an exact\nsolution if all computations are done in infinite precision. The idea is that a triangular system is easy to solve by applying backward substitution (in case of an upper\ntriangular system). How can we then reduce the given system to an equivalent triangular system?\n7.1.1\nGaussian Elimination\nWe assume that the system has a unique solution. The method reduces the system\nto an upper triangular system using elementary row operations.\n\nExample 7.1 Solve the following using Gaussian Elimination\nx1 + 2x2\n=\n−1\n=\n−2x1 −x2 + x3\n=\nSolution: The augmented matrix is\ñA =\n⎛\n⎝\n−1\n−2\n−1\n⎞\n⎠\nReducing ̃A to upper triangular gives\n⎛\n⎝\n−1\n−1\n−5\n⎞\n⎠.\nSolving by back substitution gives: x3 = 2,\nx2 = −1,\nx1 = 1. Or the solution is\nx = (1, −1, 2)T.\nThe gaussin elimination method is to systematically reduce the\nsystem (82) into a triangular system through a sequence of finite steps as described\nbelow.\nLet A(1) denote the initial augmented matrix\n(82). Re-arrange A(1) so that\na11 ̸= 0. If this is not possible, then det(A) = 0 meaning the system is singular.\nOtherwise we have\nA(1) =\n⎛\n⎜\n⎜\n⎜\n⎝\na(1)\na(1)\n1n\nb(1)\na(1)\na(1)\n2n\nb(1)\n...\n...\n...\n...\na(1)\nn1\na(1)\nn2\nnn\nb(1)\nn\n⎞\n⎟\n⎟\n⎟\n⎠\nDefine the n −1 multipliers\nm(1)\ni1 = −a(1)\ni1\na(1)\n, i > 1.", "word_count": 450, "start_char": 0, "end_char": 2121}
{"chunk_id": "DOC0112__00001", "doc_id": "DOC0112", "chunk_index": 1, "text": "therwise we have\nA(1) =\n⎛\n⎜\n⎜\n⎜\n⎝\na(1)\na(1)\n1n\nb(1)\na(1)\na(1)\n2n\nb(1)\n...\n...\n...\n...\na(1)\nn1\na(1)\nn2\nnn\nb(1)\nn\n⎞\n⎟\n⎟\n⎟\n⎠\nDefine the n −1 multipliers\nm(1)\ni1 = −a(1)\ni1\na(1)\n, i > 1.\n\n7.1\nThen reduce A(1) →A(2) using the procedure\nm(1)\ni1 R1 + Ri →Ri , i > 1\nExample 7.2 From the previous example,\nm21 = −a21\na11\n= −2\n1.\n□\nThat is, by elementary row operations, we reduce all the entries below a(1)\n11 to\nzero so that we have\nA(2) =\n⎛\n⎜\n⎜\n⎜\n⎝\na(1)\na(1)\n1n\nb(1)\na(2)\n2n\nb(2)\n...\n...\n...\n...\na(2)\nn2\nnn\nb(2)\nn\n⎞\n⎟\n⎟\n⎟\n⎠\nNext define the n −2 multipliers\nm(2)\ni2 = −a(2)\ni2\na(2)\n, i > 2,\nand then reduce A(2) →A(3) using the operations\nm(2)\ni2 R2 + Ri →Ri , i > 2.\nThis will generate\nA(3) =\n⎛\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\na(1)\na(1)\na(1)\n1n\nb(1)\na(2)\na(2)\n2n\nb(2)\na(3)\n3n\nb(3)\n...\n...\n...\n...\n...\na(3)\nn3\nnn\nb(3)\nn\n⎞\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n\nAssume a33 ̸= 0, obtain A(4) using the same procedure. Continue until you\nhave\nA(n) =\n⎛\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\na(1)\na(1)\na(1)\n1n\nb(1)\na(2)\na(2)\n2n\nb(2)\na(3)\n3n\nb(3)\n...\n...\n...\n...\n...\n· · · a(n)\nnn\nb(n)\nn\n⎞\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\nA(n) is upper triangular and det(A(n)) ̸= 0.\nDefinition 7.1 The numbers a(1)\n11 , a(2)\n22 , a(3)\n33 , . . . , a(n)\nnn are called pivot elements.\nThe system can now be solved using backsubstitution. That is,\nxn = b(n)\nn\na(n)\nnn\nand\nxi =\na(i)\nii\n'\nb(i)\ni −\nn\n\"\nj=i+1\naijxj\n(\n, i = n −1, n −2, . . ., 1.\nExercise 7.1.1-1: Solve using Gaussian Elimination:\nx1 + x2 + x3\n=\n−1\n=\n−8\n4x1 + 6x2 + 8x3\n=\nDefinition 7.2 (Pivoting) The process of swapping two rows to avoid a zero pivot is called\npivoting\nIf the pivot element is so small compared to the elements below it in the same column, we accumulate rounding errors and in the process we may end up with a\ncompletely wrong solution. Partial pivoting is used to reduce on the amount of\nrounding errors when we use Gaussian Elimination.", "word_count": 423, "start_char": 1939, "end_char": 3743}
{"chunk_id": "DOC0112__00002", "doc_id": "DOC0112", "chunk_index": 2, "text": "artial pivoting is used to reduce on the amount of\nrounding errors when we use Gaussian Elimination.\n\n7.1\nDefinition 7.3 (Partial Pivoting) The process of swapping rows to ensure that in each\ncolumn the pivot element is the largest in absolute value of all elements in that column is\ncalled partial pivoting.\nExample 7.3 Solving\n0.003x1 + 59.14x2\n=\n59.17\n5.291x1 −6.130x2\n=\n46.78\nwithout pivoting using three dcimal places yields x1 = −10.00 , x2 = 1.001, a wrong\nsolution. Such errors that arise because a computer can only store real numbers with\na finite precision are called rounding errors.\n7.1.2\nTriangular LU Decomposition\nGiven a system\nwhere A is a nonsingular matrix, we decompose/factorise A as\nA = LU\nwhere L is lower triangular and U is upper triangular. Then the system becomes\nLUx = b\n(83)\nFor a unique factorisation of A, we fix L to be unit lower triangular (with only ones on\nthe main diagonal) and the resulting triangular decomposition is more specifically\nknown as the Doolittle factorisation. Then, let\nUx = y\n(84)\nso that\nLy = b.\n(85)\nWe solve equation (85) by forward substitution to obtain y and then use y in equation (84) to obtain x by backward substitution. If A is symmetric and positive definite, then we can find the decomposition LU = A where U = Lt. This particular\ntriangular decomposition, that is, LLt = A is the Choleski decomposition of A. Further if LU = A where U is unit upper triangular, then the decomposition is called a\nCrout decomposition.\nExample 7.4 Solve using triangular decomposition\nx1 + 2x2\n=\n−1\n=\n−2x1 −x2 + x3\n=", "word_count": 279, "start_char": 3643, "end_char": 5210}
{"chunk_id": "DOC0112__00003", "doc_id": "DOC0112", "chunk_index": 3, "text": "his particular\ntriangular decomposition, that is, LLt = A is the Choleski decomposition of A.F\n\nurther if LU = A where U is unit upper triangular, then the decomposition is called a\nCrout decomposition.E\n\nxample 7.4 Solve using triangular decomposition\nx1 + 2x2\n=\n−1\n=\n−2x1 −x2 + x3\n=\n\nSolution: We have the system\n⎛\n⎝\n−2\n−1\n⎞\n⎠x =\n⎛\n⎝\n−1\n⎞\n⎠\nLet A = LU, that is\n⎛\n⎝\n−2\n−1\n⎞\n⎠=\n⎛\n⎝\nl21\nl31\nl32\n⎞\n⎠\n⎛\n⎝\nu11\nu12\nu13\nu22\nu23\nu33\n⎞\n⎠.\nMultiplying out the right hand side gives\nA =\n⎛\n⎝\nu11\nu12\nu13\nl21u11\nl21u12 + u22\nl21u13 + u23\nl31u11\nl31u12 + l32u22\nl31u13 + l32u23 + u33\n⎞\n⎠.\nBy equality of matrices, from the first row we get,\nu11 = 1 , u12 = 2 , u13 = 0.\nFrom the second row,\nl21u11 = 2\n=⇒l21 = 2,\nl21u12 + u22 = 1 =⇒u22 = 1 −(2)(2) = −3,\nl21u13 + u23 = 1 =⇒u23 = 1.\nFrom the third row,\nl31u11 = −2\n=⇒l31 = −2,\nl31u12 + l32u22 = −1, =⇒l32 = −1(−1 + 4)/3 = −1,\nl32u13 + l32u23 + u33 = 1, =⇒u33 = 2.\nHence we have\nL =\n⎛\n⎝\n−2\n−2\n−1\n⎞\n⎠and U =\n⎛\n⎝\n−3\n⎞\n⎠", "word_count": 230, "start_char": 4926, "end_char": 5878}
{"chunk_id": "DOC0112__00004", "doc_id": "DOC0112", "chunk_index": 4, "text": "rom the third row,\nl31u11 = −2\n=⇒l31 = −2,\nl31u12 + l32u22 = −1, =⇒l32 = −1(−1 + 4)/3 = −1,\nl32u13 + l32u23 + u33 = 1, =⇒u33 = 2.H\n\nence we have\nL =\n⎛\n⎝\n−2\n−2\n−1\n⎞\n⎠and U =\n⎛\n⎝\n−3\n⎞\n⎠\n\n7.2\nSolving Ly = b, that is,\n⎛\n⎝\n−2\n−1\n⎞\n⎠\n⎛\n⎝\ny1\ny2\ny3\n⎞\n⎠=\n⎛\n⎝\n−1\n⎞\n⎠,\nby forward substitution gives y1 = −1, y2 = 5 and y3 = 4.\nThen solving Ux = y, that is,\n⎛\n⎝\n−3\n⎞\n⎠\n⎛\n⎝\nx1\nx2\nx3\n⎞\n⎠=\n⎛\n⎝\n−1\n⎞\n⎠,\nby back substitution gives x3 = 2, x2 = −1 and x1 = 1, that is, x = (1, −1, 2)T.\nExercise 7.1.2-2:\nSolve by LU decomposition:\nx1\n+\n5x2\n+\n4x3\n+\n3x4\n=\n−5\n2x1\n+\n7x2\n+\n6x3\n+\n10x4\n=\n3x1\n+\n2x2\n+\n8x3\n+\n255x4\n=\n2x1\n+\nx2\n−\n2x3\n+\n27x4\n=\n7.2\nIterative methods for\n(86)\nare of the form\nx(k+1) = Gx(k) + c, k = 0, 1, 2, . . ..\n(87)\nThe matrix G is the called the iteration matrix and the vector c is called the iteration\nvector. The scheme generates a sequence of vectors x(k), k = 0, 1, 2, . . ., which if\nconverges, it converges to the solution x of the system. The principal of the schemes\ndiscussed here is the same but the difference is in how the iteration matrix G and\nvector c are formulated from A and b.", "word_count": 272, "start_char": 5695, "end_char": 6782}
{"chunk_id": "DOC0112__00005", "doc_id": "DOC0112", "chunk_index": 5, "text": "he scheme generates a sequence of vectors x(k), k = 0, 1, 2, . . ., which if\nconverges, it converges to the solution x of the system.T\n\nhe principal of the schemes\ndiscussed here is the same but the difference is in how the iteration matrix G and\nvector c are formulated from A and b.\n\n7.2.1\nJacobi Iteration Scheme\nIn this case we decompose A into\nA = L + D + U\nwhere L is the strictly lower part of A, D is the diagonal of A and U is the strictly\nupper part of A. Then (86) becomes\n(L + D + U)x = b.\n(88)\nThis can be rewritten as\n(L + U)x + Dx = b\nto give\nDx = −(L + U)x + b.\nHence we have\nx = −D−1(L + U)x + D−1b.\nWe now set\nx(k+1) = −D−1(L + U)x(k) + D−1b, k = 0, 1, 2, . . ..\n(89)\nEquation (89) is in the form\nx(k+1) = Gjx(k) + cj\nand is the Jacobi iteration scheme where\nGj = −D−1(L + U)\nis the Jacobi iteration matrix and\ncj = D−1b\nis the Jacobi iteration vector.\nExample 7.5 Solve using Jacobi iteration\n10x1\n+\nx2\n+\nx3\n=\n−x1\n+\n20x2\n+\nx3\n=\nx1\n+\n−2x2\n+\n100x3\n=\nSolution: From\nA =\n⎛\n⎝\n−1\n−2\n⎞\n⎠and b =\n⎛\n⎝\n⎞\n⎠,\n\n7.2\nwe gget\nL =\n⎛\n⎝\n−1\n−2\n⎞\n⎠,\nD =\n⎛\n⎝\n⎞\n⎠, and U =\n⎛\n⎝\n⎞\n⎠.\nWe can now form\nGj = −D−1(L + U) = −\n⎛\n⎝\n⎞\n⎠\n⎛\n⎝\n−1\n−2\n⎞\n⎠= −\n⎛\n⎝\n−1\n−2\n⎞\n⎠\nand\ncj = D−1b =\n⎛\n⎝\n⎞\n⎠\n⎛\n⎝\n⎞\n⎠=\n⎛\n⎝\n2.4\n1.05\n⎞\n⎠\nThe Jacobi iterations for the problem are then given by\nx(k+1) =\n⎛\n⎝\n−1/10\n−1/10\n1/20\n−1/20\n2/100\n⎞\n⎠x(k) +\n⎛\n⎝\n2.4\n1.05\n⎞\n⎠, k = 0, 1, 2, . . ..\n(90)\nLet\nx(0) =\n⎛\n⎝\n⎞\n⎠,\nthen\nx(1) =\n⎛\n⎝\n−1/10\n−1/10\n1/20\n−1/20\n2/100\n⎞\n⎠\n⎛\n⎝\n⎞\n⎠+\n⎛\n⎝\n2.4\n1.05\n⎞\n⎠=\n⎛\n⎝\n2.4\n1.05\n⎞\n⎠,", "word_count": 380, "start_char": 6498, "end_char": 7966}
{"chunk_id": "DOC0112__00006", "doc_id": "DOC0112", "chunk_index": 6, "text": "x(2) =\n⎛\n⎝\n−1/10\n−1/10\n1/20\n−1/20\n2/100\n⎞\n⎠\n⎛\n⎝\n2.4\n1.05\n⎞\n⎠+\n⎛\n⎝\n2.4\n1.05\n⎞\n⎠=\n⎛\n⎝\n1.995\n1.02\n2.997\n⎞\n⎠,\nx(3) =\n⎛\n⎝\n−1/10\n−1/10\n1/20\n−1/20\n2/100\n⎞\n⎠\n⎛\n⎝\n1.995\n1.02\n2.997\n⎞\n⎠+\n⎛\n⎝\n2.4\n1.05\n⎞\n⎠=\n⎛\n⎝\n1.9983\n0.9999\n3.0005\n⎞\n⎠\nContinuing this way we find that the iteration converges to:\nx =\n⎛\n⎝\n2.000\n1.000\n3.000\n⎞\n⎠.\n7.2.2\nGauss-Seidel Iteration\nIn this case, after decomposing (86) into the form (88), we write is as\n(L + D)x = −Ux + b.\nThis is rearranged to obtain\nx = −(L + D)−1Ux + (L + D)−1b.\nWe set\nx(k+1) = −(L + D)−1Ux(k) + (L + D)−1b, k = 0, 1, 2, . . ..\n(91)\nEquation (91) is in the form\nx(k+1) = Ggx(k) + cg\nand is the Gauss-Seidel iteration scheme, where,\nGg = −(L + D)−1U\nis the Gauss-Seidel iteration matrix and\ncg = (L + D)−1b\nis the Gauss-Seidel iteration vector.\nExample 7.6 Solve the previous example using Gauss Seidel iteration.\n\n7.2\nSolution:\nGg = −(L + D)−1U =\n⎛\n⎝\n−1/10\n1/10\n⎞\n⎠\nand\ncg = (L + D)−1b =\n⎛\n⎝\n2.4\n1.17\n⎞\n⎠.\nLet\nx(0) =\n⎛\n⎝\n⎞\n⎠,\nthen\nx(1) =\n⎛\n⎝\n−1/10\n1/10\n⎞\n⎠\n⎛\n⎝\n⎞\n⎠+\n⎛\n⎝\n2.4\n1.17\n⎞\n⎠=\n⎛\n⎝\n2.4\n1.17\n⎞\n⎠,\nx(2) =\n⎛\n⎝\n−1/10\n1/10\n⎞\n⎠\n⎛\n⎝\n2.4\n1.17\n⎞\n⎠+\n⎛\n⎝\n2.4\n1.17\n⎞\n⎠\nContinuing this way we find that the iteration converges to\nx =\n⎛\n⎝\n2.000\n1.000\n3.000\n⎞\n⎠.", "word_count": 285, "start_char": 7966, "end_char": 9153}
{"chunk_id": "DOC0112__00007", "doc_id": "DOC0112", "chunk_index": 7, "text": "7.3\nObserve that the iterations above are actually fixed point iterative schemes. Thus, If\nx∗is the solution then,\nx∗= Gx∗+ c.\n(92)\nSubtracting (92) from (87) gives\nx(k+1) −x∗= G(x(k) −x∗).\n(93)\nDefine\ne(k) = x(k) −x∗\nas the error in x(k). Then from (93) we have\ne(k+1) = Ge(k) = G2e(k−1) = G3e(k−2) = · · · = Gk+1e(0).\n(94)\nwhere e(0) is the error in the initial guess to the solution. This helps us prove the\nfollowing theorem\nTheorem 7.1 (Convergence if G is convergent) The iterative scheme (87) converges to\na limit with any arbitrary choice of the initial approximation x(0) if and only if G is a convergent matrix.\nProof:\nIf G is a convergent matrix, then Gn →O as n →∞, where O is the zero matrix.\nThen using this in (93) gives\ne(k+1) = Gk+1e(0) →Oe(0) = 0 as k →∞.\nHence the scheme converges. On the other hand, since e(0) is fixed at the begining\nof the scheme, then Gk+1e(0) →0 =⇒G(k+1) →O as k →∞.\n□\nThe spectral radius of the iteration matrix G is also fundamental in determining the\nconvergence of the scheme (87).\nTheorem 7.2 (Convergence if ρ(G) < 1) The iterative scheme (87) converges if and only\nif the spectral radius of G is less than 1, that is ρ(G) < 1.\nProof:\nLet\nGV = VΛ\nbe an eigen value decomposition of G, that is, Λ is a diagonal matrix of eigenvalues\nof G and V is a matrix whose columns are eigenvectors of G. For convergence, we\nhave that\ne(k) = Gke(0) →0 as k →∞.", "word_count": 269, "start_char": 9153, "end_char": 10549}
{"chunk_id": "DOC0112__00008", "doc_id": "DOC0112", "chunk_index": 8, "text": "roof:\nLet\nGV = VΛ\nbe an eigen value decomposition of G, that is, Λ is a diagonal matrix of eigenvalues\nof G and V is a matrix whose columns are eigenvectors of G.F\n\nor convergence, we\nhave that\ne(k) = Gke(0) →0 as k →∞.\n\n7.3\nThis means that Gk →O as k →∞since e(0) is fixed by the initial guess. Since Gk\nhas the same eigenvectors as G, to be precise,\nGkV = VΛk,\nthen Gk →O if and only if Λ →O. This means that λi →0 as k →∞for all i, since\nΛ is a diagonal matrix of eigenvalues of G, λi, i = 1, 2, 3, · · · , n. That is to say, we\nhave convergence if and only if |λi| < 1 ∀i. Thus we have convergence if and only\nif ρ(G) < 1.\nRate of convergence\nNow, suppose G has n eigenvectors and eigenvalues Vi and λi respectively, i =\n1, 2, 3, . . ., n. We can use the vi’s as a basis for e(0) so that\ne(0) =\nn\n\"\ni=1\nαivi,\n(95)\nfor some αi ∈R, i = 1, 2, . . ., n. Then\ne(1) = Ge(0) = G\nn\n\"\ni=1\nαivi =\nn\n\"\ni=1\nαiGvi =\nn\n\"\ni=1\nαiλivi.\nSimilarly\ne(2) = Ge(1) = G\nn\n\"\ni=1\nαiλivi =\nn\n\"\ni=1\nαiλiGvi =\nn\n\"\ni=1\nαiλ2\nivi.\nProceeding in a similar way gives\ne(k) =\nn\n\"\ni=1\nαiλk\ni vi.\n(96)\nThis further shows that e(k) →0 as k →∞if and only if λk\ni →0 as k →∞∀i. In\nparticular, the behaviour of e(k) will be dominated by the largest λi, that is, if\nρ = max\ni\n|λi|,", "word_count": 292, "start_char": 10330, "end_char": 11572}
{"chunk_id": "DOC0112__00009", "doc_id": "DOC0112", "chunk_index": 9, "text": "roceeding in a similar way gives\ne(k) =\nn\n\"\ni=1\nαiλk\ni vi.\n(96)\nThis further shows that e(k) →0 as k →∞if and only if λk\ni →0 as k →∞∀i.I\n\nn\nparticular, the behaviour of e(k) will be dominated by the largest λi, that is, if\nρ = max\ni\n|λi|,\n\nthen ρ determines the rate of convergence of the scheme. The smaller the value of ρ,\nthe faster the convergence.\nExample 7.7 How many iterations does it take to reduce the initial error by a factor\n10−3?\nSolution: Let ρ be the spectral radius of G. Then show that\ne(k) = ρke(0)\n=⇒||e(k)|| = ρk||′e(0)||.\nSo after the kth iteration, we want that ρk = 10−3, =⇒k = log 10−3/log ρ\nDefinition 7.4 The number log ρ is called the speed of convergence.\nTheorem 7.3 (Convergence due to Diagonal dominance) If A is strictly diagonally\ndominant, then the sequence of solutions produced by either Jacobi or Gauss-Seidel iteration converges to the solution of Ax = b for any x(0).\nProof: A square matrix A is said to be strictly diagonally dominant if\n|aii| >\nn\n\"\nj=1,j̸=i\n|aij| for all i = 1, 2, 3, . . ., n.\nFor Jacobi,\nG = −D−1(L + U),\nthen\n∥x∥∞= ∥L + U∥∞\n∥D∥∞\n=\nmax\n1≤i≤n\n%\nj=1,j̸=i\n|aij|\nmax\n1≤i≤n |aii|\n< 1\nsince A is strictly diagonally dominant.\nExercise 7.3-1: Check that\nA =\n⎛\n⎝\n−1\n−4\n−2\n−1\n⎞\n⎠\n\n7.3\nis strictly diagonally dominant.\nExercise 7.3-2:\n(a) Consider the iteration scheme (87). Show that ∥e(k+1)∥≤∥G∥k∥e(0)∥.\n(b) Check for the convergence of the Jacobi and Gauss-Seidel schemes for linear\nsystems whose:\n(i)\nA =\n⎛\n⎝\n⎞\n⎠,\nb =\n⎛\n⎝\n⎞\n⎠\n(ii)\nA =\n⎛\n⎝\n−1\n⎞\n⎠,\nb =\n⎛\n⎝\n⎞\n⎠\n(c) Verify that the spectral radius for Gauss-Seidel is less than that for Jacobi, that\nis, ρ(Gg) < ρ(Gj). The implication of this is that the Gauss-Seidel scheme\nconverges faster than the Jacobi scheme.", "word_count": 339, "start_char": 11333, "end_char": 13051}
